---
title: "Predicting Housing Prices"
author: "David Currie"
date: "November 21, 2016"
output: html_document
---

This is the code for my submission in Kaggle's "House Prices: Advanced Regression Techniques" competition. This competition was quite a bit of fun because of the numerous ways I could clean the data, engineer new features, and choose how to build my model. The goal for this competition was to minimize RMSLE when predicting the selling price of a house. If you would like to learn more about this competition, visit https://www.kaggle.com/c/house-prices-advanced-regression-techniques

The sections for my analysis are:
- Inspecting the Data
- Cleaning the Data (only visible in the .Rmd file)
- Feature Engineering (only visible in the .Rmd file)
- Building the Model
- Summary

```{r load packages, echo=FALSE, message=FALSE, warning=FALSE}
#Load the packages
library(ggplot2)
library(lattice)
library(MASS)
library(gridExtra)
library(memisc)
library(corrplot)
library(GGally)
library(plyr)
library(dplyr)
library(caretEnsemble)
library(devtools)
library(pbkrtest)
library(lme4)
library(ModelMetrics)
library(caret)
library(dummies)
library(regpro)
library(denpro)
library(rqPen)
library(kernlab)
library(gbm)
library(survival)
library(splines)
library(parallel)
library(xgboost)
install_github('zachmayer/caretEnsemble')

knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

```{r helper functions}
#create a histogram based on a 'feature' and label the x-axis with the feature's name
histo <- function(feature, feature_name, data) {
  ggplot(aes(feature), data = data) +
    geom_histogram(col = 'white') +
    labs(x = feature_name)
}

#create a histogram based on a 'feature', but scale the y axis by log10.
log10histo <- function(feature, feature_name, data) {
  ggplot(aes(log10(feature)), data = data) +
    geom_histogram(col = 'white') +
    labs(x = feature_name)
}

#create a scatter plot based on a 'feature', and add a linear regression line.
scatter <- function(feature, feature_name, data) {
  ggplot(aes(feature, SalePrice), data = data) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm") +
  xlab(feature_name)
}

#Create a boxplot based on a factored feature, and overlay the data points in a scatterplot.
box_jitter <- function(feature, feature_name, data) {
  ggplot(aes(factor(feature), SalePrice), data = data) +
  geom_boxplot(color = 'red') +
  geom_jitter(alpha = 0.1, color = 'blue') +
  xlab(feature_name)
}

#An outlier line that will be added to other plots. It is 3.5 standard deviations above the median.
outlier_line <- function(feature) {
  return (geom_hline(yintercept = (median(feature) + sd(feature)*3.5),
                     alpha = 1/3, linetype = 2, color = 'blue'))
}

#Create a boxplot and a histogram to show the distibution of data points. The outlier line is included in the boxplot.
box_histo <- function(feature, feature_name, data) {
  return (grid.arrange(ggplot(aes(x = 1, y = feature), data = data) + 
               geom_jitter(alpha = 0.1) +
               geom_boxplot(alpha = 0.2, color = 'red') +
               stat_summary(fun.y=mean,shape=1,col='red',geom='point') +
               outlier_line(feature) +
               ylab(feature_name),
             ggplot(aes(feature), data = data) + 
               geom_histogram(bins=30, col = 'white') +
               labs(x = feature_name), ncol=2))
}

#Creates a simple bar graph.
bar <- function(feature) {
  ggplot(aes(feature), data = train) +
    geom_bar()
}

#Creates a scatterplot with a non-linear regression line.
scatterSmooth <- function(feature, feature_name, data) {
  ggplot(aes(feature, SalePrice), data = data) +
  geom_point(alpha = 0.1) +
  geom_smooth() +
  xlab(feature_name)
}

```


```{r Load Files}
train <- read.csv("/Users/Dave/Desktop/Programming/Personal Projects/House-Prices-Kaggle/train.csv", stringsAsFactors = F)

test <- read.csv("/Users/Dave/Desktop/Programming/Personal Projects/House-Prices-Kaggle/test.csv", stringsAsFactors = F)
```

# Inspecting the Data

The dimensions of the train dataset:

```{r dimensions}
dim(train)
```

Not the biggest dataset, but that's alright.

```{r structure}
str(train)
```

We have quite a bit of missing data here, let's take a look to see how much.

```{r columns NAs}
sort(colSums(sapply(train, is.na)), decreasing = TRUE)
```

Ok, so we have quite a few missing values in some features. Let's fix that. 

**Note: This is where I have cleaned the features. If you would like to see how I performed this step, please view the .RMD file if you are not.

```{r combine train and set}
#Bind together the train and test data sets so we only have to perform each function once.
df <- rbind(select(train,Id:SaleCondition),
            select(test,Id:SaleCondition))

#Now let's double check which features we need to clean now that we have both sets of data.
#sort(colSums(sapply(df, is.na)), decreasing = TRUE)
```



```{r clean data}
#Alrighty then, let's clean these features.

#If there is a pool, change the quality to the most common - 'Gd', otherwise, 'None'.
df[df$PoolArea > 0 & is.na(df$PoolQC), "PoolQC"] <- rep("Gd") 
df$PoolQC[is.na(df$PoolQC)] <- rep('None') 

#Most common value
df$MiscFeature[is.na(df$MiscFeature)] <- rep('None')

#No alley
df$Alley[is.na(df$Alley)] <- rep('No')

#No fence
df$Fence[is.na(df$Fence)] <- rep('None') 

#There is no fireplace so set the quality to None.
df$FireplaceQu[is.na(df$FireplaceQu)] <- rep('None')

#Although it won't be perfect, there is some correlation between LotFrontage and LotArea. By taking the ratio of their means, this should provide a reasonable guess for what the LotFrontage is for the missing values.
#cor.test(df$LotFrontage, df$LotArea)
df$LotFrontage[is.na(df$LotFrontage)] <- ((mean(df$LotFrontage, na.rm = TRUE) / 
                                              mean(df$LotArea)) * df$LotArea)

#Although the data description sheet said that NA = no garage, I am going to set the value of GarageYrBuilt to the mean year a garage was built relative to the house, to provide consistency in the date. Setting it equal to 0 or the mean of GarageYearBuilt, could weaken the relationship between SalePrice and GarageYearBuilt.
df$GarageYrBlt[is.na(df$GarageYrBlt)] <- (mean(df$GarageYrBlt - df$YearBuilt, 
                                                    na.rm = TRUE) + df$YearBuilt)

#There is no garage, so I will set the values to 0.
#df[is.na(df$GarageCars),]
df$GarageCars[is.na(df$GarageCars)] <- rep(0)
df$GarageArea[is.na(df$GarageArea)] <- rep(0)

#There is one garage that is missing many values, so we'll fill them in with the most common.
#df[df$GarageArea > 0 & is.na(df$GarageFinish),]
df[df$GarageArea > 0 & is.na(df$GarageFinish), "GarageFinish"] <- rep('Unf')
df[df$GarageArea > 0 & is.na(df$GarageQual), "GarageQual"] <- rep('TA')
df[df$GarageArea > 0 & is.na(df$GarageCond), "GarageCond"] <- rep('TA')
#For the rest, they do not relate to a garage, so we'll set their values to 'None'.  
df$GarageFinish[is.na(df$GarageFinish)] <- rep('None')
df$GarageQual[is.na(df$GarageQual)] <- rep('None')
df$GarageCond[is.na(df$GarageCond)] <- rep('None')
df$GarageType[is.na(df$GarageType)] <- rep('None')

# Looks like this house doesn't have a basement, so we'll set its value to 0.
#df[is.na(df$TotalBsmtSF),] 
df$TotalBsmtSF[is.na(df$TotalBsmtSF)] <- rep(0)
#table(df$TotalBsmtSF == 0)

#There are 79 houses that do not have a basement, yet 82 houses have missing values for BsmtCond. The basements with no BsmtCond value will receive the most common value of 'TA', and the rest will receive 'No'.
df[df$TotalBsmtSF > 0 & is.na(df$BsmtCond), "BsmtCond"] <- "TA"
df$BsmtCond[is.na(df$BsmtCond)] <- rep('No')
#3 of the 82 missing BsmtExposure values relate to basements, their value will be set to the most common value - 'No', as for the others 'None' because there is no basement. 
df[df$TotalBsmtSF > 0 & is.na(df$BsmtExposure), "BsmtExposure"] <- "No"
df$BsmtExposure[is.na(df$BsmtExposure)] <- rep('None')
#2 of the 81 mssing BsmtQual values relate to basements, they value will be set to the most common value - 'TA', otherwise 'No'.
df[df$TotalBsmtSF > 0 & is.na(df$BsmtQual), "BsmtQual"] <- "TA"
df$BsmtQual[is.na(df$BsmtQual)] <- rep('No')

#df[is.na(df$BsmtFinSF1),]
#It looks like this doesn't relate to an actual basement, so we'll set its value to 0.
df$BsmtFinSF1[is.na(df$BsmtFinSF1)] <- rep(0)
df$BsmtFinSF2[is.na(df$BsmtFinSF2)] <- rep(0) #same house as above

#Use the function below to see the features for the one BsmtFinType2 that is missing its value, but relates to a basement. 
#df[df$BsmtFinSF2 > 0 & is.na(df$BsmtFinType2),]
#Since most values relating to the basement are good, I will set this one to good as well.
df[df$BsmtFinSF2 > 0 & is.na(df$BsmtFinType2), "BsmtFinType2"] <- 'GLQ'
#All other missing values will be set to 'No' because they do not relate to a basement.
df$BsmtFinType2[is.na(df$BsmtFinType2)] <- "No"
#These data points don't relate to basements.
df$BsmtFinType1[is.na(df$BsmtFinType1)] <- rep('No')
df$BsmtFinType2[is.na(df$BsmtFinType2)] <- rep('None')

#Set the missing values to none and zero because there is no veneer. 
df$MasVnrType[is.na(df$MasVnrType)] <- 'None'
df$MasVnrArea[is.na(df$MasVnrArea)] <- rep(0)

#3 of MSZoning's missing values are in 'IDOTRR' where the most common value is RM, the fourth missing value is in Mitchel where the most common MSZoning value is RL.
#df[is.na(df$MSZoning),]
df[df$Neighborhood == 'IDOTRR' & is.na(df$MSZoning), "MSZoning"] <- 'RM'
df[df$Neighborhood == 'Mitchel' & is.na(df$MSZoning), "MSZoning"] <- 'RL'
#Give missing Utilities the most common value - 'AllPub'
df$Utilities[is.na(df$Utilities)] <- 'AllPub'
#It looks like these data points relate to houses that do not have basements, so we'll set their values to 0.
#df[is.na(df$BsmtFullBath),]
df$BsmtFullBath[is.na(df$BsmtFullBath)] <- rep(0)
df$BsmtHalfBath[is.na(df$BsmtHalfBath)] <- rep(0)
#Give missing Functional values the most common - 'Typ'
df$Functional[is.na(df$Functional)] <- rep('Typ') 
#Let's give the missing values of Exterior the most common - 'VinylSd'. Often the same value is used for both 1st and 2nd.
#df[is.na(df$Exterior1st),]
df$Exterior1st[is.na(df$Exterior1st)] <- rep('VinylSd')
df$Exterior2nd[is.na(df$Exterior2nd)] <- rep('VinylSd')
#TotalBsmtSF is 0, so I will set this value to 0 as well.
#df[is.na(df$BsmtUnfSF),]
df$BsmtUnfSF[is.na(df$BsmtUnfSF)] <- rep(0)
#Give most common value - 'SBrkr'
df$Electrical[is.na(df$Electrical)] <- rep('SBrkr')
#This house doesn't seem too good, so I'll set the KitchQual value to 'TA'
#df[is.na(df$KitchenQual),]
df$KitchenQual[is.na(df$KitchenQual)] <- rep('TA')
#Set SaleType to the most common - 'WD'
df$SaleType[is.na(df$SaleType)] <- rep('WD')
```



```{r double check cleaning}
#Double check to make sure everything is alright.
#colSums(sapply(df, is.na))
#Super!
```


```{r seperate train and test sets}
#Next step, let's seperate these dataset so that we can explore the features more closely.
CleanedTrain <- df[0:nrow(train),]
CleanedTest <- df[(nrow(train) + 1): nrow(df),]
CleanedTrain$SalePrice <- train$SalePrice
```

What are the most important numerical features, based on correlation?

```{r fig.height=12, fig.width=14}
#select only numerical features
numerics <- CleanedTrain[which(sapply(CleanedTrain, is.numeric))]
numerics <- subset(numerics, select = -c(Id))
correlations <- cor(numerics)
corrplot(correlations, method="circle", order ="FPC")

```

Let's take a look at some features that are highly correlated with selling price.

```{r}
histo(CleanedTrain$GrLivArea, "GrLivArea", data = CleanedTrain)
log10histo(CleanedTrain$GrLivArea, "log10(GrLivArea)", data = CleanedTrain)
scatterSmooth(CleanedTrain$GrLivArea, "GrLivArea", CleanedTrain)
summary(CleanedTrain$GrLivArea)  
```

The distribution is a little long-tail and there are two outliers with square footage greater than 4500, and a sale price less than 200,000. Let's see how the plot changes without those points.

```{r}
ggplot(aes(GrLivArea, SalePrice), data = subset(CleanedTrain, GrLivArea < 4500)) +
  geom_point(alpha = 0.1) +
  geom_smooth()
```

That looks better, but before we remove those outliers, let's take a look at the correlation. First with the two datapoints, then without.

```{r}
#You'll notice here and in additional code blocks below that I set a equal to the dataframe that I am working on. This allows me to experiment with the data, without having to reload the full dataset if I want to undo an action or try something different.

cor.test(CleanedTrain$SalePrice, CleanedTrain$GrLivArea)
a <- subset(CleanedTrain, GrLivArea < 4500) #remove outliers
cor.test(a$SalePrice, a$GrLivArea)
#Test to see if correlation increases after a transformation.
#cor.test(a$SalePrice, log10(a$GrLivArea))
#cor.test(a$SalePrice, sqrt(a$GrLivArea))

CleanedTrain <- a
```

That should help improve the results. Let's remove those two points. 

```{r}
histo(CleanedTrain$OverallQual, "OverallQual", CleanedTrain)
ggplot(aes(factor(OverallQual), SalePrice), data = CleanedTrain) +
  geom_boxplot(color = 'red') +
  geom_jitter(alpha = 0.1, color = 'blue')
cor.test(CleanedTrain$SalePrice, CleanedTrain$OverallQual)
table(CleanedTrain$OverallQual)
```

Everything looks good with OverallQual.

Although OveralCond is not highly correlated with SalePrice, I want to have a closer look, because I thought it would have similar values to OverallQual.

```{r}
histo(CleanedTrain$OverallCond, "OverallCond", CleanedTrain)
ggplot(aes(factor(OverallCond), SalePrice), data = CleanedTrain) +
  geom_boxplot(color = 'red') +
  geom_jitter(alpha = 0.1, color = 'blue') +
  geom_smooth(aes(OverallCond), 
              method = "lm", 
              size=1, 
              color = 'green')
cor.test(CleanedTrain$SalePrice, CleanedTrain$OverallCond)
table(CleanedTrain$OverallQual)
```

I don't notice anything worrying/wrong with the data. It looks like the huge range of selling prices with OverallCond of 5 might have ruined any chance of a strong correlation. 

```{r}
histo(CleanedTrain$FullBath, "FullBath", CleanedTrain)
box_jitter(CleanedTrain$FullBath, "FullBath", CleanedTrain)
table(CleanedTrain$FullBath)
```

This looks good, but now I'm going to focus on continuous variables to see if we can find any more outliers.

```{r}
histo(CleanedTrain$X1stFlrSF, "1stFlrSF", CleanedTrain)
log10histo(CleanedTrain$X1stFlrSF, "log10(1stFlrSF)", CleanedTrain)
scatter(CleanedTrain$X1stFlrSF, "1stFlrSF", CleanedTrain)
cor.test(CleanedTrain$X1stFlrSF, CleanedTrain$SalePrice)
#cor.test(log10(CleanedTrain$X1stFlrSF), CleanedTrain$SalePrice)
#cor.test(sqrt(CleanedTrain$X1stFlrSF), CleanedTrain$SalePrice)
```

Everything looks fine with first floor square footage.

```{r}
histo(CleanedTrain$TotalBsmtSF, "TotalBsmSF", CleanedTrain)
scatter(CleanedTrain$TotalBsmtSF, "TotalBsmSF", CleanedTrain)
cor.test(CleanedTrain$TotalBsmtSF, CleanedTrain$SalePrice)
```

Everything looks good here.

```{r}
histo(CleanedTrain$YearBuilt, "YearBuilt", CleanedTrain)
scatter(CleanedTrain$YearBuilt, "YearBuilt", CleanedTrain)
cor.test(CleanedTrain$YearBuilt, CleanedTrain$SalePrice)
#cor.test(log10(CleanedTrain$YearBuilt), CleanedTrain$SalePrice)
```

It's interesting to see the housing booms and busts (~1960 and ~2013), plus everything looks fine.

```{r}
histo(CleanedTrain$LotArea, "Lot Area", CleanedTrain)
log10histo(CleanedTrain$LotArea, "log10(Lot Area)", CleanedTrain)
scatter(CleanedTrain$LotArea, "LotArea", CleanedTrain)
```

There are some big outliers here. Below you'll see the end result after I experimented with a range of subsets, between lot areas of 15,000 to the maximum value, and 25,00 seemed to be the optimal limit. I also compared the correlations between the SalePrice and other features with, and without, the outliers. Removing the outliers looks to have better or equal correlations, so we'll go ahead and remove those datapoints.

```{r}
a <- subset(CleanedTrain, LotArea < 25000)
scatter(a$LotArea, "LotArea", a)

cor.test(CleanedTrain$SalePrice, CleanedTrain$LotArea)
cor.test(a$SalePrice, a$LotArea)
#cor.test(a$SalePrice, log10(a$LotArea))
CleanedTrain <- a
```


```{r}
histo(CleanedTrain$LotFrontage, "Lot Frontage", CleanedTrain)
scatter(CleanedTrain$LotFrontage, "Lot Frontage", CleanedTrain)
cor.test(CleanedTrain$SalePrice, CleanedTrain$LotFrontage)
#I experimented with different subset below to determine the optimal dataset to include. It turns out that using the full dataset is the best.
#a <- subset(CleanedTrain, LotFrontage < 900)
#scatter(a$LotFrontage, "Lot Frontage", a)
#cor.test(a$SalePrice, a$LotFrontage)
#b <- subset(CleanedTrain, LotFrontage < 200)
#scatter(b$LotFrontage, "Lot Frontage", b)
#cor.test(b$SalePrice, b$LotFrontage)
#cor.test(b$SalePrice, b$LotArea)

#CleanedTrain <- a
```

Although this is a significant outlier, my model performs better with this datapoint, so I will leave it in.

```{r}
histo(CleanedTrain$X2ndFlrSF, "2ndFlrSF", CleanedTrain)
scatter(CleanedTrain$X2ndFlrSF, "2ndFlrSF", CleanedTrain)
cor.test(CleanedTrain$SalePrice, CleanedTrain$X2ndFlrSF)
```


Many houses do not have second floors. The data looks fine.

```{r}
histo(CleanedTrain$WoodDeckSF, "WoodDeckSF", CleanedTrain)
scatter(CleanedTrain$WoodDeckSF, "WoodDeckSF", CleanedTrain)
cor.test(CleanedTrain$SalePrice, CleanedTrain$WoodDeckSF)
```

Many people also do not have wooddecks, but again, the data looks fine.

```{r}
ggplot(aes(factor(Neighborhood), SalePrice), data = CleanedTrain) +
  geom_boxplot(color = 'red') +
  geom_jitter(alpha = 0.1, color = 'blue') +
  xlab("Neighborhood") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
table(CleanedTrain$Neighborhood)
```

There are definitely some more expensive neighborhoods, such as NoRidge and NridgHt. We'll use this information for the feature engineering section.

```{r}
scatter(CleanedTrain$GarageArea, "GarageArea", CleanedTrain)
cor.test(CleanedTrain$GarageArea, CleanedTrain$SalePrice)
```

Hmm, let's see what happens if we remove values greater than 1248.

```{r}
a <- subset(CleanedTrain, GarageArea < 1248)
scatter(a$GarageArea, "GarageArea", a)
cor.test(a$GarageArea, a$SalePrice)
```

It's only a slight improvement, so we'll keep the datapoints to have more information for build our model.

```{r}
CleanedTrain$MonthYearSold <- CleanedTrain$YrSold + (CleanedTrain$MoSold/12)
CleanedTest$MonthYearSold <- CleanedTest$YrSold + (CleanedTest$MoSold/12)
scatter(CleanedTrain$MonthYearSold, "MonthYearSold", CleanedTrain)
cor.test(CleanedTrain$MonthYearSold, CleanedTrain$SalePrice)
```

Given that this dataset takes place during the recession, I was wondering if the selling prices would drop during 2008-2010...they didn't.

```{r}
histo(CleanedTrain$MSSubClass, "MS Sub Class", CleanedTrain)
box_jitter(CleanedTrain$MSSubClass, "MS Sub Class", CleanedTrain)
table(CleanedTrain$MSSubClass)
```

It's a little tough to see any strong insights here. 2-STORY 1946 & NEWER (#60) houses are generally worth the most, but so are 1-STORY PUD (Planned Unit Development, #120) - 1946 & NEWER. Perhaps the number of stories doesn't matter as much as when the house was made.

```{r}
box_jitter(CleanedTrain$HouseStyle, "HouseStyle", CleanedTrain)
table(CleanedTrain$HouseStyle)
```

Just as a reminder, the correlation between sale price and year built is 0.5487, so I am confident in saying that year built has a stronger correlation with sale price than house style / number of stories.

```{r}
histo(CleanedTrain$YearRemodAdd, "YearRemodAdd", CleanedTrain)
scatter(CleanedTrain$YearRemodAdd, "YearRemodAdd", CleanedTrain)
```

The earliest value for YearRemodAdd:
```{r}
min(CleanedTrain$YearRemodAdd)
```

The number of houses with this minimum value:
```{r}
table(CleanedTrain$YearRemodAdd == 1950)
```

```{r}
cor.test(CleanedTrain$SalePrice, CleanedTrain$YearRemodAdd)
#cor.test(CleanedTrain$SalePrice, log10(CleanedTrain$YearRemodAdd))
```


There are way too many houses that have their value for YearRemodAdd as 1950. I am going to assume that this is the earliest date possible for this value, which has led to the error. For houses with YearRemodAdd = 1950, I am going to change their value to the average difference between when the house was built and remodelled, plus the year the house was built. Here's an example to clear the confusion: YearBuilt = 1930, average difference between year built and remodelled = 4.38, new value for YearRemodAdd = 1934.38.

```{r}

a <- CleanedTrain
#Select only the years after 1950, because we know for the house built in 1950 or earlier the difference will be 0.
DifYearRemodandBuilt <- mean(a[a$YearBuilt > 1950, "YearRemodAdd"] - 
                             a[a$YearBuilt > 1950, "YearBuilt"])
a[a$YearBuilt < 1950, "YearRemodAdd"] <- a[a$YearBuilt < 1950, "YearBuilt"] +
  DifYearRemodandBuilt
#table(a$YearRemodAdd)
scatter(a$YearRemodAdd, "YearRemodAdd", a)
cor.test(a$SalePrice, a$YearRemodAdd)

CleanedTrain <- a
```

Although the correlation went down, I believe these new values more accurately represent the real world.

Before we moved onto feature engineering, it would definitely be worth having a look at the sale prices.

```{r}
box_histo(CleanedTrain$SalePrice, "Sale Price", CleanedTrain)
```

I've been seeing the two data points with a sale price of over $700,000 in many graphs. After looking at the quality of my model, I've decided to also remove the datapoints with a sale price greater than $600,000.

```{r}
CleanedTrain <- subset(CleanedTrain, SalePrice < 600000)
```


Now let's bring our train and test dataset back together to do some feature engineering. **Note: just like with cleaning the data, if you want to see the steps I took, please use the .Rmd file.

```{r}
a <- subset(CleanedTrain, select = -c(SalePrice))
df2 <- rbind(select(a,MSSubClass:MonthYearSold),
            select(CleanedTest,MSSubClass:MonthYearSold))
```



```{r}
#First, let's change all of the ranking variables from 'Ex', 'Gd', etc to numbers: 4,3,2,1,0.
df2[df2$ExterQual == "Ex", "ExterQual"] <-4
df2[df2$ExterQual == "Gd", "ExterQual"] <-3
df2[df2$ExterQual == "TA", "ExterQual"] <-2
df2[df2$ExterQual == "Fa", "ExterQual"] <-1
df2$ExterQual <- strtoi(df2$ExterQual)

df2[df2$ExterCond == "Ex", "ExterCond"] <-4
df2[df2$ExterCond == "Gd", "ExterCond"] <-3
df2[df2$ExterCond == "TA", "ExterCond"] <-2
df2[df2$ExterCond == "Fa", "ExterCond"] <-1
df2[df2$ExterCond == "Po", "ExterCond"] <-0
df2$ExterCond <- strtoi(df2$ExterCond)

df2[df2$BsmtQual == "Ex", "BsmtQual"] <-4
df2[df2$BsmtQual == "Gd", "BsmtQual"] <-3
df2[df2$BsmtQual == "TA", "BsmtQual"] <-2
df2[df2$BsmtQual == "Fa", "BsmtQual"] <-1
df2[df2$BsmtQual == "No", "BsmtQual"] <-0
df2$BsmtQual <- strtoi(df2$BsmtQual)

df2[df2$BsmtCond == "Gd", "BsmtCond"] <-3
df2[df2$BsmtCond == "TA", "BsmtCond"] <-2
df2[df2$BsmtCond == "Fa", "BsmtCond"] <-1
df2[df2$BsmtCond == "Po", "BsmtCond"] <-0
df2[df2$BsmtCond == "No", "BsmtCond"] <-0
df2$BsmtCond <- strtoi(df2$BsmtCond)

df2[df2$BsmtExposure == "Gd", "BsmtExposure"] <-3
df2[df2$BsmtExposure == "Av", "BsmtExposure"] <-2
df2[df2$BsmtExposure == "Mn", "BsmtExposure"] <-1
df2[df2$BsmtExposure == "None", "BsmtExposure"] <-0
df2[df2$BsmtExposure == "No", "BsmtExposure"] <-0
df2$BsmtExposure <- strtoi(df2$BsmtExposure)

df2[df2$BsmtFinType1 == "GLQ", "BsmtFinType1"] <-3
df2[df2$BsmtFinType1 == "ALQ", "BsmtFinType1"] <-2
df2[df2$BsmtFinType1 == "Rec", "BsmtFinType1"] <-2
df2[df2$BsmtFinType1 == "BLQ", "BsmtFinType1"] <-1
df2[df2$BsmtFinType1 == "LwQ", "BsmtFinType1"] <-0
df2[df2$BsmtFinType1 == "Unf", "BsmtFinType1"] <-0
df2[df2$BsmtFinType1 == "No", "BsmtFinType1"] <-0
df2$BsmtFinType1 <- strtoi(df2$BsmtFinType1)

df2[df2$BsmtFinType2 == "GLQ", "BsmtFinType2"] <-3
df2[df2$BsmtFinType2 == "ALQ", "BsmtFinType2"] <-2
df2[df2$BsmtFinType2 == "Rec", "BsmtFinType2"] <-2
df2[df2$BsmtFinType2 == "BLQ", "BsmtFinType2"] <-1
df2[df2$BsmtFinType2 == "LwQ", "BsmtFinType2"] <-0
df2[df2$BsmtFinType2 == "Unf", "BsmtFinType2"] <-0
df2[df2$BsmtFinType2 == "No", "BsmtFinType2"] <-0
df2$BsmtFinType2 <- strtoi(df2$BsmtFinType2)

df2[df2$HeatingQC == "Ex", "HeatingQC"] <-4
df2[df2$HeatingQC == "Gd", "HeatingQC"] <-3
df2[df2$HeatingQC == "TA", "HeatingQC"] <-2
df2[df2$HeatingQC == "Fa", "HeatingQC"] <-1
df2[df2$HeatingQC == "Po", "HeatingQC"] <-0
df2$HeatingQC <- strtoi(df2$HeatingQC)

df2[df2$KitchenQual == "Ex", "KitchenQual"] <-4
df2[df2$KitchenQual == "Gd", "KitchenQual"] <-3
df2[df2$KitchenQual == "TA", "KitchenQual"] <-2
df2[df2$KitchenQual == "Fa", "KitchenQual"] <-1
df2$KitchenQual <- strtoi(df2$KitchenQual)

df2[df2$FireplaceQu == "Ex", "FireplaceQu"] <-4
df2[df2$FireplaceQu == "Gd", "FireplaceQu"] <-3
df2[df2$FireplaceQu == "TA", "FireplaceQu"] <-2
df2[df2$FireplaceQu == "Fa", "FireplaceQu"] <-1
df2[df2$FireplaceQu == "Po", "FireplaceQu"] <-0
df2[df2$FireplaceQu == "None", "FireplaceQu"] <-0
df2$FireplaceQu <- strtoi(df2$FireplaceQu)

df2[df2$GarageQual == "Ex", "GarageQual"] <-4
df2[df2$GarageQual == "Gd", "GarageQual"] <-3
df2[df2$GarageQual == "TA", "GarageQual"] <-2
df2[df2$GarageQual == "Fa", "GarageQual"] <-1
df2[df2$GarageQual == "Po", "GarageQual"] <-0
df2[df2$GarageQual == "None", "GarageQual"] <-0
df2$GarageQual <- strtoi(df2$GarageQual)

df2[df2$GarageCond == "Ex", "GarageCond"] <-4
df2[df2$GarageCond == "Gd", "GarageCond"] <-3
df2[df2$GarageCond == "TA", "GarageCond"] <-2
df2[df2$GarageCond == "Fa", "GarageCond"] <-1
df2[df2$GarageCond == "Po", "GarageCond"] <-0
df2[df2$GarageCond == "None", "GarageCond"] <-0
df2$GarageCond <- strtoi(df2$GarageCond)

df2[df2$PoolQC == "Ex", "PoolQC"] <-4
df2[df2$PoolQC == "Gd", "PoolQC"] <-3
df2[df2$PoolQC == "Fa", "PoolQC"] <-1
df2[df2$PoolQC == "None", "PoolQC"] <-0
df2$PoolQC <- strtoi(df2$PoolQC)

df2[df2$Fence == "GdPrv", "Fence"] <-3
df2[df2$Fence == "GdWo", "Fence"] <-3
df2[df2$Fence == "MnWw", "Fence"] <-1
df2[df2$Fence == "MnPrv", "Fence"] <-1
df2[df2$Fence == "None", "Fence"] <-0
df2$Fence <- strtoi(df2$Fence)

#All good there, now let's make some new features!
```



```{r add new features}
#What percent of the lot is taken up by the house (represented by the first floor's square footage)
df2$percentHouse <- df2$X1stFlrSF / df2$LotArea

#What percent of the lot is taken up by the house plus the garage.
df2$percentHouseGarage <- (df2$X1stFlrSF + df2$GarageArea) / df2$LotArea

#These are the neighborhoods with the highest sale prices.
df2$GoodNeighborhood <- 0
df2[df2$Neighborhood == "NoRidge", "GoodNeighborhood"] <- 1
df2[df2$Neighborhood == "NridgHt", "GoodNeighborhood"] <- 1
df2[df2$Neighborhood == "StoneBr", "GoodNeighborhood"] <- 1

#These are the neighborhoods with the lowest sale prices.
df2$BadNeighborhood <- 0
df2[df2$Neighborhood == "BrDale", "BadNeighborhood"] <- 1
df2[df2$Neighborhood == "MeadowV", "BadNeighborhood"] <- 1
df2[df2$Neighborhood == "IDOTRR", "BadNeighborhood"] <- 1
df2[df2$Neighborhood == "Blueste", "BadNeighborhood"] <- 1
df2[df2$Neighborhood == "NPkVill", "BadNeighborhood"] <- 1
df2[df2$Neighborhood == "OldTown", "BadNeighborhood"] <- 1
df2[df2$Neighborhood == "SawyerW", "BadNeighborhood"] <- 1
df2[df2$Neighborhood == "SWISU", "BadNeighborhood"] <- 1

#The year difference between when the house was built and when it was remodelled.
df2$DifYearRemodel <- df2$YearBuilt - df2$YearRemodAdd

#Total all the quality features to calculate the 'TotalQuality'.
df2$TotalQuality <- (df2$OverallQual + df2$ExterQual + df2$BsmtQual + df2$HeatingQC +
                     df2$KitchenQual + df2$FireplaceQu + df2$GarageQual + df2$PoolQC +
                     df2$Fence)

#Total all the Condition features.
df2$TotalCond <- (df2$OverallCond + df2$ExterCond + df2$BsmtCond + df2$GarageCond)

#Total the quality of the basement features.
df2$BsmtTotalQuality <- df2$BsmtQual + df2$BsmtFinType1 + df2$BsmtFinType2

#Total the quality of the main features that are inside the house.
df2$InsideQuality <- df2$OverallQual + df2$BsmtQual + df2$KitchenQual + df2$GarageQual

#An artifical feature that multiplies the overall quality by the overall condition.
df2$OverallQualxCond <- df2$OverallQual * df2$OverallCond

#Total square footage of the basement
df2$BsmtTotalFinSF <- df2$TotalBsmtSF - df2$BsmtUnfSF

#Total square footage of the house, without the garage.
df2$TotalInsideSF <- df2$X1stFlrSF + df2$X2ndFlrSF + df2$TotalBsmtSF

#Total square footage of the house, plus the garage.
df2$HouseandGarageSF <- df2$TotalInsideSF + df2$GarageArea

#Percent of the basement's square footage that is finished.
df2$BsmtFinPercentSF <- (1 - (df2$BsmtUnfSF/df2$TotalBsmtSF)) 
df2$BsmtFinPercentSF[is.na(df2$BsmtFinPercentSF)] <- rep(0)

#Total square footage of the house's features.
df2$TotalSF <- (df2$TotalInsideSF + df2$PoolArea + df2$ScreenPorch + df2$X3SsnPorch +
                df2$EnclosedPorch + df2$OpenPorchSF + df2$WoodDeckSF + df2$GarageArea) 

#The square footage that is unused by the house's features.
df2$UnusedSF <- df2$LotArea - df2$TotalSF

#Total bathrooms that are in the house.
df2$TotalBaths <- df2$BsmtFullBath + (df2$BsmtHalfBath/2) + df2$FullBath + (df2$HalfBath/2)

#Number of bathrooms per square foot of the house.
df2$TotalBathsperSF <- df2$TotalBaths / df2$TotalInsideSF

#An artifical feature that emphasizes the importance of many, good fireplaces.
df2$FireplacesxQual <- df2$FireplaceQu * df2$Fireplaces

#An artifical feature that emphasizes the importance of many, good kitchens.
df2$KitchensxQual <- df2$KitchenAbvGr * df2$KitchenQual

#An artifical feature that emphasizes the importance of size and quality of the house.
df2$SFxQual <- df2$TotalSF * df2$TotalQual

#Number of rooms above ground.
df2$TotalRoomsAboveG <- df2$TotRmsAbvGrd + df2$FullBath + df2$HalfBath

#Number of common rooms above ground.
df2$TotalCommonRoomsAboveG <- df2$TotRmsAbvGrd - df2$BedroomAbvGr

#Number of basic rooms, i.e. not bedrooms, kitchens, or bathrooms. *Bathrooms are exculded from TotRmsAbvGrd.
df2$BasicRoomsAboveG <- df2$TotRmsAbvGrd - df2$BedroomAbvGr - df2$KitchenAbvGr

#Percentage of the square footage that is low quality
df2$LQPErcentSF <- df2$LowQualFinSF / df2$TotalInsideSF

#The number of square feet in the garage per car slot.
df2$GarageSFperCar <- df2$GarageArea / df2$GarageCars
df2$GarageSFperCar[is.na(df2$GarageSFperCar)] <- rep(0)

#The square footage of the house's outdoor features.
df2$OutsideSF <- (df2$WoodDeckSF + df2$OpenPorchSF + df2$EnclosedPorch +
                  df2$X3SsnPorch + df2$ScreenPorch)

#The difference between when the house was sold and when it was remodelled.
df2$DifYearRemodelandSold <- df2$YrSold - df2$YearRemodAdd

#The difference between when the house was built and sold.
df2$DifYearBuiltandSold <- (df2$YrSold - df2$YearBuilt)

#The difference between when the house was remodelled and built.
df2$DifYearBuiltandRemod <- df2$YearRemodAdd - df2$YearBuilt

#The total quality of the basement's finished areas.
df2$BsmtFinType1and2 <- df2$BsmtFinType1 + df2$BsmtFinType2

#How large the second floor is relative to the first floor.
df2$Flr2percentofFlr1 <- df2$X2ndFlrSF / df2$X1stFlrSF

#How large the first floor is relative to the house.
df2$percentFl1 <- df2$X1stFlrSF / df2$TotalInsideSF

#How old the garage is relative to the house.
df2$recentGarage <- df2$GarageYrBlt - df2$YearBuilt
df2$recentGarage[is.na(df2$recentGarage)] <- rep(0)

#Average size of a room above ground.
df2$AvgAboveGRoomSize <- (df2$X1stFlrSF + df2$X2ndFlrSF) / (df2$TotRmsAbvGrd +
                                                            df2$FullBath + df2$HalfBath)

#The size of the lot relative to the lot frontage.
df2$LotAreaoverFrontage <- df2$LotArea / df2$LotFrontage

#The total square footage of the lot and house. It double-counts the square footage of the first floor and garage, but this is to overemphasize the importance of larger houses.
df2$LotandHouseSF <- df2$LotArea + df2$HouseandGarageSF

#An artifical feature that emphasizes the importance of large, good basements.
df2$BsmtSFxQual <- df2$TotalBsmtSF * df2$BsmtQual

#If the house has a second story
df2$Has2Stories <- 0
df2[df2$X2ndFlrSF > 0, "Has2Stories"] <- 1

#If the house has a basement
df2$HasBasement <- 0
df2[df2$TotalBsmtSF > 0, "HasBasement"] <- 1

#If the house has a pool
df2$HasPool <- 0
df2[df2$PoolArea > 0, "HasPool"] <- 1

#Age of the house
df2$Age <- max(df2$YearBuilt) - df2$YearBuilt 

#Simplify OverallQual into bad, average, good.
df2[df2$OverallQual == 1, "SimpleOverallQual"] <- 1
df2[df2$OverallQual == 2, "SimpleOverallQual"] <- 1
df2[df2$OverallQual == 3, "SimpleOverallQual"] <- 1
df2[df2$OverallQual == 4, "SimpleOverallQual"] <- 2
df2[df2$OverallQual == 5, "SimpleOverallQual"] <- 2
df2[df2$OverallQual == 6, "SimpleOverallQual"] <- 2
df2[df2$OverallQual == 7, "SimpleOverallQual"] <- 2
df2[df2$OverallQual == 8, "SimpleOverallQual"] <- 3
df2[df2$OverallQual == 9, "SimpleOverallQual"] <- 3
df2[df2$OverallQual == 10, "SimpleOverallQual"] <- 3

#Simplify OverallCond into bad, average, good.
df2[df2$OverallCond == 1, "SimpleOverallCond"] <- 1
df2[df2$OverallCond == 2, "SimpleOverallCond"] <- 1
df2[df2$OverallCond == 3, "SimpleOverallCond"] <- 1
df2[df2$OverallCond == 4, "SimpleOverallCond"] <- 2
df2[df2$OverallCond == 5, "SimpleOverallCond"] <- 2
df2[df2$OverallCond == 6, "SimpleOverallCond"] <- 2
df2[df2$OverallCond == 7, "SimpleOverallCond"] <- 2
df2[df2$OverallCond == 8, "SimpleOverallCond"] <- 3
df2[df2$OverallCond == 9, "SimpleOverallCond"] <- 3
df2[df2$OverallCond == 10, "SimpleOverallCond"] <- 3

#Simpify Lot Area into small, medium, large.
df2[df2$LotArea <= 8000, "SimpleLotArea"] <- 1
df2[df2$LotArea > 8000, "SimpleLotArea"] <- 2
df2[df2$LotArea > 10500, "SimpleLotArea"] <- 3

#Simpify Year Built into old, medium, new.
df2[df2$YearBuilt <= 1960, "SimpleYearBuilt"] <- 1
df2[df2$YearBuilt > 1960, "SimpleYearBuilt"] <- 2
df2[df2$YearBuilt > 1990, "SimpleYearBuilt"] <- 3

#Simpify YearRemodAdd into old, medium, new.
df2[df2$YearRemodAdd <= 1970, "SimpleYearRemodAdd"] <- 1
df2[df2$YearRemodAdd > 1970, "SimpleYearRemodAdd"] <- 2
df2[df2$YearRemodAdd > 1998, "SimpleYearRemodAdd"] <- 3

#Simpify TotalInsideSF into small, average, large.
df2[df2$TotalInsideSF <= 2100, "SimpleTotalInsideSF"] <- 1
df2[df2$TotalInsideSF > 2100, "SimpleTotalInsideSF"] <- 2
df2[df2$TotalInsideSF > 2750, "SimpleTotalInsideSF"] <- 3

#Sort Month Sold into seasons.
df2[df2$MoSold == 1, "SeasonSold"] <- 1
df2[df2$MoSold == 2, "SeasonSold"] <- 1
df2[df2$MoSold == 3, "SeasonSold"] <- 1
df2[df2$MoSold == 4, "SeasonSold"] <- 2
df2[df2$MoSold == 5, "SeasonSold"] <- 2
df2[df2$MoSold == 6, "SeasonSold"] <- 2
df2[df2$MoSold == 7, "SeasonSold"] <- 3
df2[df2$MoSold == 8, "SeasonSold"] <- 3
df2[df2$MoSold == 9, "SeasonSold"] <- 3
df2[df2$MoSold == 10, "SeasonSold"] <- 4
df2[df2$MoSold == 11, "SeasonSold"] <- 4
df2[df2$MoSold == 12, "SeasonSold"] <- 4


#That should be a good amount of new feature, plus I've ran out of good ideas. 
```




```{r}
#Let's create dummy variables for the character features.

#select features by their class
feature_classes <- sapply(names(df2),function(x){class(df2[[x]])})
#select features with the class 'character'.
categorical_features <- names(feature_classes[feature_classes == "character"])
#create dummy variables
dummies <- dummyVars(~.,df2[categorical_features])
dummy_features <- predict(dummies,df2[categorical_features])
#if dummy variable = na, set it to 0
dummy_features[is.na(dummy_features)] <- 0
#select features that do not have the class 'character'.
numeric_features <-names(feature_classes[feature_classes != "character"])
#combine all of the features.
dfFinal <- cbind(df2[numeric_features], dummy_features)
#RoofMatlMetal was removed after observing the importance of the variables in the model. This feature has no importance.
dfFinal <- subset(dfFinal, select = -c(RoofMatlMetal))
```


```{r}
#Let's reseperate the datasets and have a look at these new features.
FinalTrain <- dfFinal[0:nrow(CleanedTrain),]
FinalTest <- dfFinal[(nrow(CleanedTrain) + 1): nrow(dfFinal),]
FinalTrain$SalePrice <- CleanedTrain$SalePrice
```


```{r}
#separate the training data into a training and testing set.
set.seed(2)
partition <- createDataPartition(FinalTrain$SalePrice, p=0.8, list = FALSE)
training <- FinalTrain[partition,]
testing <- FinalTrain[-partition,]
```

Now it's time to train the model.

# Building the Model

```{r}
#Use KFold for cross validation.
control <- trainControl(method="cv",
                        number = 3, #3 folds
                        repeats = 1, #repeats the data once.
                        savePredictions= 'final', 
                        classProbs=TRUE,
                        verbose = TRUE
                        )

#The algorithms have been tuned to their optimal values.
tunes <- list(rqnc = caretModelSpec(method="rqnc", preProcess = c('zv'),
                                    tuneGrid=data.frame(.lambda=c(0.000005),
                                                        .penalty = 'MCP')),
              rqlasso = caretModelSpec(method="rqlasso", preProcess = c('zv'),
                                       tuneGrid=data.frame(.lambda = c(0.002))),
              svmLinear = caretModelSpec(method="svmLinear", preProcess = c('zv'),
                                       tuneGrid=data.frame(.C = c(0.6)))
              )

models <- caretList(SalePrice ~ ., data=training, tuneList = tunes, trControl=control,
                    metric = "RMSE")
```

These algorithms were chosen after doing spot checks on their initial performance, then their parameters were tuned. 

```{r}
results <- resamples(models)
summary(results)
dotplot(results)
modelCor(results)
```

Ensemble the models together.

```{r}
#build an ensemble model.
ensemble <- caretEnsemble(models, metric="RMSE",
                          trControl=trainControl(number=10, repeats = 3, classProbs=TRUE)
                         )
ensemble$error
summary(ensemble)
plot(ensemble)
```

Summary of input algorithms, then the ensembled model:

```{r}
preds <- data.frame(sapply(models, predict, newdata=testing))
predsEnsemble <- predict(ensemble, newdata=testing)
summary(preds)
summary(predsEnsemble)
```

```{r}
print("RMSLE of the testing values")
rmse(log(testing$SalePrice), log(predsEnsemble))
```


First ten predicted housing prices of the dataset to be submitted for competition.

```{r}
predsFinal <- predict(ensemble, FinalTest)
solution <- data.frame(Id=as.integer(rownames(FinalTest)),SalePrice=predsFinal)
write.csv(solution,"/Users/Dave/Desktop/Programming/Personal Projects/House-Prices-Kaggle/submission3.csv",row.names=FALSE)
head(solution, 10)
```

# Summary

Although this was not the largest dataset, there were still some challenges and clever thinking required to do well in this competition. I believed that I have done a good job cleaning the data, creating new features, and building my ensemble model because I currently rank in the top 16% of submissions. 

Below you can see a summary of my predicted selling prices, and ten most important features. As expected, square footage and quality play a large role in the selling price of a house. It was neat to see the importance of feature engineering, especially creating artifical features (such as SFxQual), in order to build a accurate model. 

```{r}
histogram(predsFinal)
summary(predsFinal)
```


```{r}
imp <- sort(varImp(ensemble, scale = FALSE), decreasing = TRUE)
importantFeatures <- rownames(imp)[1:20]
head(imp,10)
```






